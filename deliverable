# CMPSC 472 – Operating Systems (Section 001)

**Professor:** Janghoon Yang  
**Author:** Jomiloju Odumosu  

## Project 1: MapReduce Systems for Parallel Sorting and Max-Value Aggregation with Constrained Memory

**GitHub Link for Code:** [https://github.com/jto5108/cmpsc472-project1](https://github.com/jto5108/cmpsc472-project1)

---

## 1. Project Description

### Overview of the Two MapReduce-Style Tasks

In this project, I implemented two MapReduce-style systems on a single machine: **Parallel Sorting** and **Max-Value Aggregation with Constrained Shared Memory**. Both tasks follow the MapReduce model:

- **Map phase:** data is processed in parallel by several workers.  
- **Reduce phase:** results from workers are combined by a reducer.

**Parallel Sorting:**  
- Input: Large array of integers.  
- Map phase: Array divided into chunks, each sorted by a worker (threads or processes).  
- Reduce phase: Merges sorted chunks into a final array.  

**Max-Value Aggregation:**  
- Each worker computes the local maximum of its chunk.  
- Workers share a single memory buffer storing one value (global max).  
- Synchronization ensures only one worker updates the value at a time.

### Why I Considered Multithreading, Multiprocessing, and Synchronization

- **Multithreading:** Lightweight, shared memory access; limited by Python’s GIL.  
- **Multiprocessing:** True parallelism across cores; requires IPC.  
- **Synchronization:** Ensures correctness when updating shared data, preventing race conditions.

---

## 2. Instructions

### Parallel Sorting
```bash
-python mapreduce_sort.py --mode thread --workers 4 --size 131072
-python mapreduce_sort.py --mode process --workers 8 --size 32

### Max-Value Aggregation
-python max_aggregation.py --mode thread --workers 4 --size 131072
-python max_aggregation.py --mode process --workers 8 --size 32


### Parameters:

--mode: thread or process

--workers: number of workers (1, 2, 4, or 8)

--size: size of input array

### Outputs:

-Mode, number of workers, input size

-Map time, reduce time, total time

-Memory usage before and after execution

-Final output (sorted list or global max)

### Testing:

-Small arrays (size 32) for correctness

-Large arrays (size 131,072) for performance evaluation

### 3. Structure of the Code
### Parallel Sorting System Diagram
Main Process
 ├── Worker 1 (Thread/Process): Sort Chunk 1
 ├── Worker 2 (Thread/Process): Sort Chunk 2
 ├── Worker 3 (Thread/Process): Sort Chunk 3
 └── Worker N (Thread/Process): Sort Chunk N
      Reducer (Main): Merge all sorted chunks via heapq.merge


-Threads share array directly

-Processes send sorted chunks via multiprocessing.Queue

Max-Value Aggregation System Diagram
Main Process
 ├── Worker 1 --> local_max --> try update shared buffer
 ├── Worker 2 --> local_max --> try update shared buffer
 ├── Worker 3 --> local_max --> try update shared buffer
 └── Worker N --> local_max --> try update shared buffer
      Reducer (Main): Reads final global_max from shared buffer


-Threads: shared list [value] + threading.Lock

-Processes: multiprocessing.Value + Lock

## MapReduce Support:

-Map phase: parallel workers independently process data

-Intermediate results: sorted chunks or local maxima

-Reduce phase: main process merges or aggregates results

-Mimics MapReduce without distributed infrastructure

### 4. Implementation Details

## Libraries and Tools:

 -The implementation was entirely written in Python using only standard libraries, except for psutil (used for measuring memory usage).
 -Below is a summary of the libraries and their roles:

  -threading – used to create lightweight threads for shared-memory parallelism.

  -multiprocessing – used to create separate processes for true parallelism across CPU cores.

  -heapq – used to merge multiple sorted chunks efficiently during the reduce phase of the sorting task.

  -argparse – used for command-line argument handling, making the scripts flexible to run with different modes, sizes, and worker counts.

  -time – used to measure execution times for each phase (map, reduce, total).

  -random – used to generate random integer arrays for input data.

  -psutil – used to record memory usage before and after execution.

  -typing – added for cleaner function signatures and readability.
##Process and Thread Management

-In my project, I used both threads and processes to parallelize work depending on the selected mode.

-For thread-based parallelism, I relied on the threading module in Python. Each worker was created manually using threading.Thread, where I could pass the worker’s function and its arguments (i.e., start index, end index, and the shared result list). After starting all threads, I used the .join() method to make sure the main thread waited for all workers to complete before moving on to the reduce phase. This ensured proper synchronization and avoided partial results.

-Threads run in the same memory space, which made data sharing simple — workers directly accessed portions of the same input list without needing inter-process communication. However, due to Python’s Global Interpreter Lock (GIL), true CPU parallelism is limited. Despite that, I still observed performance improvements in I/O-like or chunk-isolated tasks like sorting since each worker could operate independently.

-For process-based parallelism, I used multiprocessing.Process to achieve real parallelism across multiple CPU cores. Each process has its own separate memory space, so workers cannot directly share variables. To handle this, I used IPC (Inter-Process Communication) through a multiprocessing.Queue or multiprocessing.Value.

-For sorting, each worker process sorted its assigned chunk and then sent the sorted result back to the main process through a Queue.

-For max aggregation, workers wrote updates to a single shared multiprocessing.Value, which acted as the global maximum.

-Each process was started, executed its assigned work, and then joined. This manual control helped me manage worker lifecycle precisely and ensured deterministic execution order during testing.

##IPC Mechanisms

-Since processes do not share memory by default, I had to carefully choose suitable IPC (Inter-Process Communication) mechanisms to exchange data efficiently.

#For the Sorting Task:

-I used a multiprocessing.Queue, which allows multiple processes to send Python objects (in this case, sorted lists) back to the main process safely. Each worker placed its sorted chunk into the queue, and the reducer collected all chunks by reading from it. The Queue automatically handles serialization (pickling/unpickling), which simplified data transfer between processes.

-Although this introduces slight overhead compared to shared memory, it made the design more robust and modular — each process worked independently and communicated only through the queue, similar to how distributed MapReduce workers would exchange intermediate results in a cluster environment.

#For the Max Aggregation Task:

-I used a multiprocessing.Value, a shared memory object that stores a single variable accessible by multiple processes. In my case, it stored the current global maximum. Each worker calculated its local maximum and, with a lock, compared and updated the shared Value if its local maximum was greater.

-This approach simulates constrained shared memory conditions where only one global value can be updated, and synchronization ensures consistency across processes. It was a simple but effective design that demonstrated race condition prevention through locking.

#Threading Strategy

-I chose manual thread creation rather than using a pre-built thread pool (like concurrent.futures.ThreadPoolExecutor) because I wanted full visibility and control over how data chunks were divided and assigned. This also allowed me to precisely measure how thread count affected timing and memory usage.

-Each thread was responsible for sorting or scanning a specific portion of the data, determined by the chunk indices I computed using a helper function. This gave me flexibility to adjust chunk sizes and number of workers easily without changing the underlying thread logic.

-Using manual threads also made it easier to handle synchronization and result collection explicitly, which was useful for understanding low-level behavior of Python threads in a parallel system.

#Synchronization Strategy

-Synchronization was critical to ensure data correctness and consistency, especially in the Max-Value Aggregation task where multiple workers attempted to modify the same shared variable concurrently.

-Threads: I used a threading.Lock to create a critical section where only one thread could check and update the shared maximum at a time. This prevented race conditions — situations where two threads might read and update the shared variable simultaneously, causing incorrect results.

-Processes: For multiprocessing, I used multiprocessing.Lock, which works across process boundaries. This ensured that even though processes run in separate memory spaces, only one process could update the shared multiprocessing.Value at a time.

-The locks were always acquired using a context manager (with lock:), which automatically handles acquiring and releasing, reducing the chance of deadlocks.

-Although synchronization slightly increased execution time when many workers were active, it guaranteed correctness, which was the main goal. Without it, test results often produced inconsistent global maxima.

-Overall, the synchronization design in my implementation achieved a balance between parallelism and safety. It demonstrated how even simple locks can maintain correctness in a concurrent environment but also how they can limit scalability when contention increases.

### 5. Performance Evaluation
-Correctness Checks (Input Size = 32)

-Both tasks produced correct outputs matching Python’s built-in sorted() and max()

-Execution times <0.002s, memory changes negligible

-Insert small-size execution output here:
    -mapreduce_sort.py --mode thread --workers 2 --size 32 MODE=thread workers=2 size=32 map_time_s: 0.000413 reduce_time_s: 0.000025 total_time_s: 0.000439 mem_before_mb: 15.50 mem_after_mb: 15.75 delta_mb: 0.25 
    -mapreduce_sort.py --mode thread --workers 8 --size 131072 MODE=thread workers=8 size=131072 map_time_s: 0.018060 reduce_time_s: 0.026952 total_time_s: 0.045013 mem_before_mb: 20.50 mem_after_mb: 22.73 delta_mb: 2.23
    -max_aggregation.py --mode thread --workers 1 --size 32 MODE=thread workers=1 size=32 map_time_s (incl updates): 0.000236 total_time_s: 0.000236 mem_before_mb: 15.38 mem_after_mb: 15.50 delta_mb: 0.12 global_max: 990382744
    -max_aggregation.py --mode thread --workers 4 --size 32 MODE=thread workers=4 size=32 map_time_s (incl updates): 0.000598 total_time_s: 0.000599 mem_before_mb: 15.38 mem_after_mb: 15.50 delta_mb: 0.12 global_max: 812525487

##Performance Assessment (Input Size = 131,072) 

  -I next ran performance tests on a much larger dataset (131,072 integers). 
  -Below are the summarized results for the thread-based version of both tasks. 

##Parallel Sorting Results 

| **Workers** | **Map (s)** | **Reduce (s)** | **Total (s)** | **Δ Mem (MB)** |
| ----------- | ----------- | -------------- | ------------- | -------------- |
| 1           | 0.039111    | 0.010033       | 0.049146      | +2.89          |
| 2           | 0.019386    | 0.017016       | 0.036403      | +2.77          |
| 4           | 0.017827    | 0.021309       | 0.039138      | +2.33          |
| 8           | 0.018060    | 0.026952       | 0.045013      | +2.23          |

##Max-Value Aggregation Results
| **Workers** | **Map + Update (s)** | **Total (s)** | **Δ Mem (MB)** | **Global Max** |
| ----------- | -------------------- | ------------- | -------------- | -------------- |
| 1           | 0.004333             | 0.004333      | +0.20          | 999985138      |
| 2           | 0.004172             | 0.004173      | +0.64          | 999984119      |
| 4           | 0.004170             | 0.004171      | +0.39          | 999991328      |
| 8           | 0.004563             | 0.004564      | +0.25          | 999981169      |

##Timing Comparisons and Discussion 

#Parallel Sorting 

-For sorting, I noticed that going from 1 to 2 workers significantly reduced total time from 0.049 s to 0.036 s, showing a real benefit from parallelization. 
 -However, increasing the number of threads beyond 2 did not continue to improve speed. 
 -At 4 and 8 workers, the total time slightly increased again (0.039 s → 0.045 s), likely due to synchronization overhead and the Global Interpreter Lock (GIL) limiting true parallel execution in CPU-bound operations. 

-Memory usage grew modestly (around 2–3 MB increase), which is expected as each worker holds its own sorted subarray before merging. 

#Max-Value Aggregation 

-The aggregation task showed very stable timing across all worker counts. 
 -Even at 8 threads, execution time stayed around 0.0045 seconds. 
 -Because each thread had to acquire a lock to update the shared global maximum, more threads introduced slight contention but no major slowdown. 
 -This consistency shows that synchronization worked correctly and efficiently for the simple shared-integer update model. 

##Synchronization Discussion 

-Synchronization played a critical role in the Max-Value task. 
 -Without a lock, multiple threads would race to update the global maximum, producing inconsistent or incorrect results. 
 -Using a threading.Lock ensures that only one thread updates the value at a time, maintaining accuracy. 
 -However, it also revealed how synchronization limits scalability—beyond 4 threads, the performance gain flattened due to the sequential nature of the critical section. 

-For sorting, synchronization was minimal since each thread worked on independent chunks, but the GIL still prevented true multi-core parallelism, explaining why performance didn’t continue improving with more threads. 

 

### 6. Conclusion 

##Key Findings 

-From my results, I found that multithreading improved performance compared to a single thread when using a small number of workers, especially for the sorting task. 
 -However, increasing threads beyond 2 to 4 did not continue to improve performance because of Python’s GIL and the overhead of managing multiple threads. 
 -For aggregation, the results were highly consistent across all worker counts, proving that my synchronization design-maintained correctness while keeping the system lightweight. 

-I also observed that memory growth remained small (usually 2–3 MB for large inputs), showing that my approach was efficient in both time and space. 

## Challenges Faced 

-Some challenges I faced included: 

  -Managing synchronization correctly to avoid race conditions when updating the shared global maximum. 

  -Balancing the number of threads to find an optimal point between speedup and overhead. 

  Measuring execution time precisely for small workloads where timing differences were minimal. 

  Dealing with Python’s inherent threading limitations on CPU-bound tasks. 

##Limitations and Possible Improvements 

-One limitation is that all experiments were done using threads only. Running the same tasks with multiprocessing would show true parallel execution across cores. 
 -Future improvements could include: 

    -Implementing a multiprocessing version for comparison. 

    -Using shared memory arrays or multiprocessing.Queue to reduce data transfer overhead. 

    -Adding CPU utilization tracking to quantify parallel efficiency. 

    -Integrating thread pools (concurrent.futures.ThreadPoolExecutor) for more scalable and cleaner worker management. 

-Overall, this project deepened my understanding of how MapReduce-style computation, parallelism, and synchronization interact in practice. 
 -It showed me that parallel speedup depends not just on splitting work but also on how efficiently threads or processes share and synchronize data. 

 
